{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32629, 39)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use pandas to load data into a DataFrame\n",
    "# df = pd.read_csv(\"/home/danielbudi/Collage/comp-gammafest/comp-model/comp-dataset/mapped-v2_imputed_mf5iter.csv\")\n",
    "# df = pd.read_csv(\"/home/danielbudi/Collage/comp-gammafest/comp-model/comp-dataset/encode data/train.csv\")\n",
    "df = pd.read_csv(\"imputed_mf10iter_drop_outliers.csv\")\n",
    "df.shape # (rows, columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_COLUMN = 'DC201'\n",
    "df_sampling = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURE_LIST = list(df.columns)\n",
    "NUMERICAL_DATA = ['DC216', 'DC220', 'DC142a']\n",
    "CATEGORICAL_DATA = [column for column in df.columns if column != TARGET_COLUMN and column not in NUMERICAL_DATA]\n",
    "\n",
    "COLUMN_CATEGORICAL_INDEX = []\n",
    "\n",
    "for column in CATEGORICAL_DATA:\n",
    "    COLUMN_CATEGORICAL_INDEX.append(df.columns.get_loc(column))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-Processing Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Feature and Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_df = df[TARGET_COLUMN]\n",
    "# label = pd.DataFrame(label)\n",
    "df = df.drop(TARGET_COLUMN, axis=1)\n",
    "feature_df = df\n",
    "\n",
    "label_sampling = df_sampling[TARGET_COLUMN]\n",
    "# label = pd.DataFrame(label)\n",
    "df_sampling = df_sampling.drop(TARGET_COLUMN, axis=1)\n",
    "feature_sampling = df_sampling"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from collections import Counter\n",
    "# from imblearn.over_sampling import SMOTENC\n",
    "\n",
    "# counter = Counter(label_sampling)\n",
    "# print(counter)\n",
    "\n",
    "# oversample = SMOTENC(sampling_strategy=0.2,\n",
    "#                      categorical_features=COLUMN_CATEGORICAL_INDEX,\n",
    "#                      random_state=42)\n",
    "# feature_sampling, label_sampling = oversample.fit_resample(feature_sampling, label_sampling)\n",
    "\n",
    "# counter = Counter(label_sampling)\n",
    "# print(counter)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Importance"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Non-SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# from xgboost import XGBClassifier\n",
    "# from xgboost import plot_importance\n",
    "\n",
    "# # fit model no training data\n",
    "# model = XGBClassifier()\n",
    "# model.fit(feature_df, label_df)\n",
    "# # feature importance\n",
    "# print(model.feature_importances_)\n",
    "# # plot\n",
    "# fig, ax = plt.subplots(figsize=(10,10))\n",
    "# plot_importance(model, ax=ax)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.sort(feature_df['DC205'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature_df.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature_df_importance = feature_df[['DC024', 'DC142a', 'DC220', 'DC214', 'DC213', 'DC216',\n",
    "#                                     'DC205', 'DC235', 'DC270a', 'DC252', 'DC215', 'DC226', 'DC217']]\n",
    "# feature_df_importance"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # fit model no training data\n",
    "# model = XGBClassifier()\n",
    "# model.fit(feature_sampling, label_sampling)\n",
    "# # feature importance\n",
    "# print(model.feature_importances_)\n",
    "# # plot\n",
    "# fig, ax = plt.subplots(figsize=(10,10))\n",
    "# plot_importance(model, ax=ax)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.sort(feature_sampling['DC205'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature_sampling.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature_sampling_importance = feature_sampling[['DC024', 'DC142a', 'DC220', 'DC214', 'DC213', 'DC216', 'DC109',\n",
    "#                                                 'DC205', 'DC235', 'DC270a', 'DC252', 'DC215', 'DC226', 'DC230a']]\n",
    "# feature_sampling_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('../datasets/test.csv')\n",
    "test_id = test_df.pop('id')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import category_encoders as ce\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "# scaler = StandardScaler()\n",
    "\n",
    "# train set\n",
    "numerical_df_train = feature_sampling[NUMERICAL_DATA].astype(float).copy()\n",
    "categorical_df_train = feature_sampling[CATEGORICAL_DATA].astype('category').copy()\n",
    "\n",
    "# normalization\n",
    "scaler.fit(numerical_df_train)\n",
    "numerical_df_train = scaler.transform(numerical_df_train)\n",
    "numerical_df_train = pd.DataFrame(numerical_df_train, columns=NUMERICAL_DATA)\n",
    "\n",
    "\n",
    "# test set\n",
    "numerical_df_test = test_df[NUMERICAL_DATA].astype(float).copy()\n",
    "categorical_df_test = test_df[CATEGORICAL_DATA].astype('category').copy()\n",
    "\n",
    "# normalization\n",
    "numerical_df_test = scaler.transform(numerical_df_test)\n",
    "numerical_df_test = pd.DataFrame(numerical_df_test, columns=NUMERICAL_DATA)\n",
    "\n",
    "# Create an instance of the OneHotEncoder\n",
    "encoder = ce.OneHotEncoder(cols=CATEGORICAL_DATA, use_cat_names=True)\n",
    "\n",
    "# Fit the encoder on the training data\n",
    "encoder.fit(categorical_df_train)\n",
    "\n",
    "one_hot_df_train = encoder.transform(categorical_df_train)\n",
    "one_hot_df_test = encoder.transform(categorical_df_test)\n",
    "\n",
    "merged_df_train = pd.concat([numerical_df_train, one_hot_df_train], axis=1)\n",
    "merged_df_test = pd.concat([numerical_df_test, one_hot_df_test], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merged_df_train\n",
    "# merged_df_test"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "TEST_SIZE = 0.2\n",
    "RANDOM_SEED = 42\n",
    "FOLD = StratifiedKFold(n_splits=3, shuffle=True, random_state=RANDOM_SEED)\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X_train_norm, label_sampling, test_size=TEST_SIZE, random_state=RANDOM_SEED) # All Normalized\n",
    "X_train, X_test, y_train, y_test = train_test_split(merged_df_train, label_sampling, test_size = TEST_SIZE, random_state = RANDOM_SEED) # Numeric Normalized\n",
    "# X_train, X_test, y_train, y_test = train_test_split(feature_sampling, label_sampling, test_size=TEST_SIZE, random_state=RANDOM_SEED) # No Normalized\n",
    "# X_train, X_test, y_train, y_test = train_test_split(feature_df, label_df, test_size=TEST_SIZE, random_state=RANDOM_SEED) # No Normalized\n",
    "# X_train, X_test, y_train, y_test = feature_sampling, feature_df, label_sampling, label_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "# from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining parameter range\n",
    "# param_grid = {'learning_rate': [0.1, 0.01, 0.001],\n",
    "#               'min_child_weight':[1, 2, 3],\n",
    "#               'gamma':[0, 0.1, 0.001]}\n",
    "\n",
    "# grid = GridSearchCV(estimator=xgb,\n",
    "#                     param_grid=param_grid,\n",
    "#                     scoring='f1_micro', n_jobs=-1,\n",
    "#                     cv=FOLD, verbose=3)\n",
    "  \n",
    "# fitting the model for grid search\n",
    "# grid.fit(X_train, y_train) # Split the training and validation\n",
    "# grid.fit(X_train_norm, label) # All the training with normalized\n",
    "# grid.fit(X_resample, label) # All the training with numeric normalized\n",
    "# grid.fit(feature, label) # All the features no normalized\n",
    "\n",
    "# print(grid.best_params_)\n",
    "\n",
    "# grid_predictions = grid.best_estimator_.predict(X_test)\n",
    "# print(classification_report(y_test, grid_predictions, digits=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Projects\\comp-gammafest\\venv\\Lib\\site-packages\\lightgbm\\engine.py:177: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 23438, number of negative: 2665\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002530 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 549\n",
      "[LightGBM] [Info] Number of data points in the train set: 26103, number of used features: 156\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.897904 -> initscore=2.174155\n",
      "[LightGBM] [Info] Start training from score 2.174155\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Projects\\comp-gammafest\\venv\\Lib\\site-packages\\lightgbm\\engine.py:177: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 23404, number of negative: 2699\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002662 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 548\n",
      "[LightGBM] [Info] Number of data points in the train set: 26103, number of used features: 156\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.896602 -> initscore=2.160026\n",
      "[LightGBM] [Info] Start training from score 2.160026\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Projects\\comp-gammafest\\venv\\Lib\\site-packages\\lightgbm\\engine.py:177: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 23374, number of negative: 2729\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002279 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 543\n",
      "[LightGBM] [Info] Number of data points in the train set: 26103, number of used features: 155\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.895453 -> initscore=2.147689\n",
      "[LightGBM] [Info] Start training from score 2.147689\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Projects\\comp-gammafest\\venv\\Lib\\site-packages\\lightgbm\\engine.py:177: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 23432, number of negative: 2671\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004045 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 547\n",
      "[LightGBM] [Info] Number of data points in the train set: 26103, number of used features: 156\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.897675 -> initscore=2.171650\n",
      "[LightGBM] [Info] Start training from score 2.171650\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Projects\\comp-gammafest\\venv\\Lib\\site-packages\\lightgbm\\engine.py:177: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 23416, number of negative: 2688\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002506 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 547\n",
      "[LightGBM] [Info] Number of data points in the train set: 26104, number of used features: 155\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.897027 -> initscore=2.164622\n",
      "[LightGBM] [Info] Start training from score 2.164622\n",
      "Average F1 score: 0.9519\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Create a LightGBM dataset\n",
    "data = lgb.Dataset(merged_df_train, label=label_sampling)\n",
    "\n",
    "# Set the parameters for the LightGBM model\n",
    "params = {\n",
    "    \"objective\": \"binary\",\n",
    "    \"metric\": \"binary_logloss\",\n",
    "    \"boosting_type\": \"dart\",\n",
    "    \"n_estimators\": 1000, \n",
    "    \"subsample\": 0.8,\n",
    "    # \"olsample_bytree\": 0.8,\n",
    "    \"scale_pos_weight\":2, \n",
    "    \"num_leaves\": 100,\n",
    "    \"random_state\": 42, \n",
    "    \"learning_rate\": 0.1, \n",
    "    \"min_child_weight\":2, \n",
    "    \"max_depth\": 22\n",
    "}\n",
    "\n",
    "# Set the number of folds for cross-validation\n",
    "n_folds = 5\n",
    "\n",
    "# Initialize a KFold object\n",
    "kf = KFold(n_splits=n_folds, shuffle=True, random_state=0)\n",
    "\n",
    "# Initialize a list to store the F1 score of each fold\n",
    "f1_list = []\n",
    "\n",
    "# Perform cross-validation\n",
    "for train_index, test_index in kf.split(merged_df_train):\n",
    "    # Split the data into training and test sets\n",
    "    X_train, X_test = merged_df_train.iloc[train_index], merged_df_train.iloc[test_index]\n",
    "    y_train, y_test = label_sampling.iloc[train_index], label_sampling.iloc[test_index]\n",
    "\n",
    "    # Create a LightGBM dataset for the training data\n",
    "    train_data = lgb.Dataset(X_train, label=y_train)\n",
    "\n",
    "    # Train the model\n",
    "    model = lgb.train(params, train_data)\n",
    "\n",
    "    # Make predictions on the test set\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Convert the predicted probabilities to binary labels\n",
    "    y_pred = [round(x) for x in y_pred]\n",
    "\n",
    "    # Calculate the F1 score for this fold\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    f1_list.append(f1)\n",
    "\n",
    "# Calculate the average F1 score across all folds\n",
    "mean_f1 = sum(f1_list) / n_folds\n",
    "\n",
    "print(f\"Average F1 score: {mean_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.72065   0.26370   0.38612       675\n",
      "           1    0.92083   0.98821   0.95333      5850\n",
      "\n",
      "    accuracy                        0.91326      6525\n",
      "   macro avg    0.82074   0.62595   0.66972      6525\n",
      "weighted avg    0.90013   0.91326   0.89465      6525\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prediction = model.predict(X_test)\n",
    "result = []\n",
    "\n",
    "for pred in prediction:\n",
    "  result.append(1 if pred >= 0.5 else 0)\n",
    "print(classification_report(y_test, result, digits=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.70916   0.26370   0.38445       675\n",
      "           1    0.92078   0.98752   0.95299      5850\n",
      "\n",
      "    accuracy                        0.91264      6525\n",
      "   macro avg    0.81497   0.62561   0.66872      6525\n",
      "weighted avg    0.89889   0.91264   0.89417      6525\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lgbm = LGBMClassifier(boosting_type='dart', n_estimators=1000, subsample=0.8,\n",
    "                      colsample_bytree=0.8, scale_pos_weight=2, num_leaves=100,\n",
    "                      random_state=42, learning_rate=0.1, min_child_weight=2, max_depth=22)\n",
    "lgbm.fit(X_train, y_train)\n",
    "\n",
    "prediction = lgbm.predict(X_test)\n",
    "print(classification_report(y_test, prediction, digits=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best threshold: 0.5\n",
      "Best F1 score: 0.9529858132629495\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7092    0.2637    0.3844       675\n",
      "           1     0.9208    0.9875    0.9530      5850\n",
      "\n",
      "    accuracy                         0.9126      6525\n",
      "   macro avg     0.8150    0.6256    0.6687      6525\n",
      "weighted avg     0.8989    0.9126    0.8942      6525\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Make probability predictions on the validation data\n",
    "y_prob = lgbm.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Compute the F1 score for different threshold values\n",
    "thresholds = np.arange(0, 1.01, 0.01)\n",
    "f1_scores = [f1_score(y_test, y_prob > t) for t in thresholds]\n",
    "\n",
    "# Find the threshold that gives the highest F1 score\n",
    "best_threshold = thresholds[np.argmax(f1_scores)]\n",
    "best_f1_score = np.max(f1_scores)\n",
    "\n",
    "print(f'Best threshold: {best_threshold}')\n",
    "print(f'Best F1 score: {best_f1_score}')\n",
    "\n",
    "# Make binary predictions using the best decision threshold\n",
    "y_pred = y_prob > best_threshold\n",
    "\n",
    "print(classification_report(y_test,y_pred, digits=4))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,\n",
       "       1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=int64)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# grid_predictions = grid.best_estimator_.predict(test_norm)\n",
    "# grid_predictions = grid.best_estimator_.predict(test_df1)\n",
    "# grid_predictions = xgb.predict(test_norm)\n",
    "grid_predictions = lgbm.predict(merged_df_test)\n",
    "# grid_predictions = model.predict(test_copy)\n",
    "grid_predictions[-100:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Threshold\n",
    "\n",
    "# P_prod_submit = lgbm.predict_proba(test_norm)[:, 1] > best_threshold\n",
    "# P_prod_submit = grid.best_estimator_.predict(X_test_submit)\n",
    "# P_prod_submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>DC201</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>26718</td>\n",
       "      <td>Layak Minum</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>26802</td>\n",
       "      <td>Layak Minum</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>41302</td>\n",
       "      <td>Layak Minum</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>38698</td>\n",
       "      <td>Layak Minum</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>44257</td>\n",
       "      <td>Layak Minum</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11985</th>\n",
       "      <td>36943</td>\n",
       "      <td>Layak Minum</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11986</th>\n",
       "      <td>33415</td>\n",
       "      <td>Layak Minum</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11987</th>\n",
       "      <td>41998</td>\n",
       "      <td>Layak Minum</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11988</th>\n",
       "      <td>41567</td>\n",
       "      <td>Layak Minum</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11989</th>\n",
       "      <td>45296</td>\n",
       "      <td>Layak Minum</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11990 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id        DC201\n",
       "0      26718  Layak Minum\n",
       "1      26802  Layak Minum\n",
       "2      41302  Layak Minum\n",
       "3      38698  Layak Minum\n",
       "4      44257  Layak Minum\n",
       "...      ...          ...\n",
       "11985  36943  Layak Minum\n",
       "11986  33415  Layak Minum\n",
       "11987  41998  Layak Minum\n",
       "11988  41567  Layak Minum\n",
       "11989  45296  Layak Minum\n",
       "\n",
       "[11990 rows x 2 columns]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = []\n",
    "for res in grid_predictions:\n",
    "# for res in P_prod_submit:\n",
    "  result.append('Layak Minum' if res==1 else 'Tidak Layak Minum')\n",
    "\n",
    "finish_pd = pd.DataFrame({'id':test_id.values, 'DC201':result})\n",
    "finish_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DC201\n",
       "Layak Minum          11546\n",
       "Tidak Layak Minum      444\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finish_pd['DC201'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finish_pd.to_csv('/home/danielbudi/Collage/comp-gammafest/comp-model/comp-dataset/result-file/result-LGBM-one_hot_encode-no_SMOTE-MinMax-Cat_modus_num_imputation.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
